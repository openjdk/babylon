I will cover the KernelContext later…
The s32Array would be a sequence of memory with an S32 (int) ‘length’ field followed by a length S32. 
So in C we might describe this as a struct 

    struct {
        int length;
        int array[length];
    }

So the way to decode these is look at the interface S32Array which inherits from Array1D.  Inside Array1D we define the ‘layout’ as

    JAVA_INT.withName("length"),
    MemoryLayout.sequenceLayout(length, memoryLayout).withName("array")

Where memoryLayout is passed in from S32Array as JAVA_INT.
So as if we explicitly wrote

    JAVA_INT.withName("length"),
    MemoryLayout.sequenceLayout(length, JAVA_INT).withName("array")

Which is the Panama layout way of describing the above struct.  Make sense?
OK now the KernelContext.  This is a little weird. We have a  Java class representing the KernelContext so the Java code can ‘express’ intent (kernel asks for kc.x)  but we pass an iface mapped memory segment (also called KernelContext) to the backend. 
The segment class  is

    public int x;
    final public int maxX;

    public KernelContext(NDRange ndRange, int maxX, int x) {
        this.ndRange = ndRange;
        this.maxX = maxX;
        this.x = x;
    }

And the matching iface mapped segment is

    KernelContext kernelContext = SegmentMapper.of(lookup, KernelContext.class,
        JAVA_INT.withName("x"),
        JAVA_INT.withName("maxX")
    );

These names/fields must match :wink:  the order is not really important.
So in Java code the kernel refers to its  ‘current id’ via kc.x and the max number of id’s executing as kc.maxX.   
Aside …. we use x to leave the option of 2D ranges (so kc.maxY and kc.y), I have not decided whether to support 2D or 3D ranges yet.   
Anyway. When the code ends up on the GPU each vendor has it’s own way to represent ‘current id’.
So although we we pre populate the maxX field before dispatch. which remains constant (or uniform as they say in GPU compute) across all kernels.      
For the equiv of kx.x  we defer to how each backend extracts this runtime information.
So for example at the top of a generated C99 OpenCL kernel we inject the line

    kc.x = get_global_id(0); // <-- OpenCL’s way of determining the kernel’s ‘id’

For CUDA we inject

    kc.x = blockIdx.x*blockDim.x+threadIdx.x; // CUDAs way of determining the kernel id

See CudaHatKernelBuilder and OpenCLHatKernelBuilder which both inherit from the same base class (most of the generated code are the same for CUDA and OpenCL) but each ‘override’

    public CudaHatKernelBuilder globalId() {
        return identifier("blockIdx").dot().identifier("x")
                .asterisk()
                .identifier("blockDim").dot().identifier("x")
                .plus()
                .identifier("threadIdx").dot().identifier("x");
    }

    public OpenCLHatKernelBuilder globalId() {
        return identifier("get_global_id").oparen().literal(0).cparen();
    }

So in the PTX spec :wink: there will be an instruction representing accessing the PTX notion of ‘id’
When I look in the docs/ptx.md file I see

    mov.u32         %r8, %ntid.x;
    mov.u32         %r9, %ctaid.x;
    mov.u32         %r10, %tid.x;
    mad.lo.s32      %r1, %r9, %r8, %r10;
    mov.u32         %r11, %nctaid.x;
    mul.lo.s32      %r12, %r11, %r8;

It looks like the register names %ntid.x, %ctaid.x and %tid.x represent the values at runtime,
So you will need to generate a sequence similar to the code above to extract the kernels id

---

Here is how you dump ptx for C99 kernel code.
Assume we have `squares.cu`

    typedef struct KernelContext_s{
        int x;
        int maxX;
    }KernelContext_t;
    typedef struct S32Array_s{
        int length;
        int array[1];
    }S32Array_t;
    extern "C" __global__ void squareKernel(KernelContext_t *kc,  S32Array_t* s32Array){
        kc->x=blockIdx.x*blockDim.x+threadIdx.x;
        if(kc->x<kc->maxX){
            int value = s32Array->array[(long)kc->x];
            s32Array->array[(long)kc->x]=value*value;
        }
        return;
    }

We use

    nvcc -ptx squares.cu -o squares.ptx

Then

    cat squares.ptx
    //
    // Generated by NVIDIA NVVM Compiler
    //
    // Compiler Build ID: CL-30672275
    // Cuda compilation tools, release 11.5, V11.5.119
    // Based on NVVM 7.0.1
    //
    
    .version 7.5
    .target sm_52
    .address_size 64
    
        // .globl	squareKernel
    
    .visible .entry squareKernel(
    .param .u64 squareKernel_param_0,
    .param .u64 squareKernel_param_1
    )
    {
    .reg .pred 	%p<2>;
    .reg .b32 	%r<8>;
    .reg .b64 	%rd<7>;
    
    
        ld.param.u64 	%rd2, [squareKernel_param_0];
        ld.param.u64 	%rd1, [squareKernel_param_1];
        cvta.to.global.u64 	%rd3, %rd2;
        mov.u32 	%r2, %ntid.x;
        mov.u32 	%r3, %ctaid.x;
        mov.u32 	%r4, %tid.x;
        mad.lo.s32 	%r1, %r3, %r2, %r4;
        st.global.u32 	[%rd3], %r1;
        ld.global.u32 	%r5, [%rd3+4];
        setp.ge.s32 	%p1, %r1, %r5;
        @%p1 bra 	$L__BB0_2;
    
        cvta.to.global.u64 	%rd4, %rd1;
        mul.wide.s32 	%rd5, %r1, 4;
        add.s64 	%rd6, %rd4, %rd5;
        ld.global.u32 	%r6, [%rd6+4];
        mul.lo.s32 	%r7, %r6, %r6;
        st.global.u32 	[%rd6+4], %r7;
    
    $L__BB0_2:
    ret;
    
    }

nvcc has some more options.

    nvcc -ptx --generate-line-info --source-in-ptx squares.cu -o squares.ptx

Giving
    // .globl	squareKernel
    
    .visible .entry squareKernel(
    .param .u64 squareKernel_param_0,
    .param .u64 squareKernel_param_1
    )
    {
    .reg .pred 	%p<2>;
    .reg .b32 	%r<8>;
    .reg .b64 	%rd<7>;
    
    //squares.cu:12 extern "C" __global__ void squareKernel( KernelContext_t *kc,  S32Array_t* s32Array){
    .loc	1 12 0
    $L__func_begin0:
    
    //squares.cu:12 extern "C" __global__ void squareKernel( KernelContext_t *kc,  S32Array_t* s32Array){
    .loc	1 12 0
    
    
        ld.param.u64 	%rd2, [squareKernel_param_0];
        ld.param.u64 	%rd1, [squareKernel_param_1];
    $L__tmp0:
    
    //squares.cu:13     kc->x=blockIdx.x*blockDim.x+threadIdx.x;
    .loc	1 13 5
    cvta.to.global.u64 	%rd3, %rd2;
    mov.u32 	%r2, %ntid.x;
    mov.u32 	%r3, %ctaid.x;
    mov.u32 	%r4, %tid.x;
    mad.lo.s32 	%r1, %r3, %r2, %r4;
    st.global.u32 	[%rd3], %r1;
    
    //squares.cu:14     if(kc->x<kc->maxX){
    .loc	1 14 5
    ld.global.u32 	%r5, [%rd3+4];
    setp.ge.s32 	%p1, %r1, %r5;
    @%p1 bra 	$L__BB0_2;
    
    
    //squares.cu:13     kc->x=blockIdx.x*blockDim.x+threadIdx.x;
    .loc	1 13 5
    cvta.to.global.u64 	%rd4, %rd1;
    $L__tmp1:
    
    //squares.cu:15         int value = s32Array->array[(long)kc->x];
    .loc	1 15 19
    mul.wide.s32 	%rd5, %r1, 4;
    add.s64 	%rd6, %rd4, %rd5;
    ld.global.u32 	%r6, [%rd6+4];
    
    //squares.cu:16         s32Array->array[(long)kc->x]=value*value;
    .loc	1 16 9
    mul.lo.s32 	%r7, %r6, %r6;
    st.global.u32 	[%rd6+4], %r7;
    $L__tmp2:
    
    $L__BB0_2:
    
    //squares.cu:18     return;
    .loc	1 18 5
    ret;
    $L__tmp3:
    $L__func_end0:

---

There are two kernel parameter ‘types’ uniform primitives (int, float etc) and iface mapped memory segments.
Because of the following rules.
We disallow (or should) the creation of iface mapped segments in kernel code.
We disallow access to iface mapped buffers via fields (static or otherwise) of any class (including the containing class).
We probably should disallow iface buffer aliassing. …
->
The only iface mapped buffers we will see accessed in a kernel (or any method reachable from a kernel), were those passed via the args passed to the kernel entrypoint.
This means that we know the layout of all buffers based on the args.